# chunk_size_optimizer.py
# å·¥å…·æ¥æµ‹è¯•å’Œä¼˜åŒ– chunk size

import ollama
import numpy as np
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt

class ChunkSizeOptimizer:
    """ä¼˜åŒ– RAG ç³»ç»Ÿçš„ chunk size"""
    
    def __init__(self):
        self.embedding_model = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'
        
    def create_chunks(self, text: str, chunk_size: int, overlap: int = 50) -> List[str]:
        """åˆ›å»ºä¸åŒå¤§å°çš„ chunks"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œç»“æŸ
            if end < len(text):
                # å¯»æ‰¾æœ€è¿‘çš„å¥å·ã€é—®å·æˆ–æ„Ÿå¹å·
                for i in range(end, max(start, end-50), -1):
                    if text[i] in '.!?':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= len(text):
                break
                
        return chunks
    
    def analyze_chunk_sizes(self, documents: List[str], chunk_sizes: List[int]) -> Dict:
        """åˆ†æä¸åŒ chunk sizes çš„æ•ˆæœ"""
        results = {}
        
        print("ğŸ” åˆ†æä¸åŒ chunk sizes...")
        
        for chunk_size in chunk_sizes:
            print(f"\nğŸ“ æµ‹è¯• chunk size: {chunk_size}")
            
            all_chunks = []
            for doc in documents:
                chunks = self.create_chunks(doc, chunk_size)
                all_chunks.extend(chunks)
            
            # åˆ†æç»Ÿè®¡æ•°æ®
            chunk_lengths = [len(chunk) for chunk in all_chunks]
            word_counts = [len(chunk.split()) for chunk in all_chunks]
            
            # æµ‹è¯• embedding æ•ˆæœ
            embedding_success = 0
            embedding_times = []
            
            for chunk in all_chunks[:5]:  # æµ‹è¯•å‰5ä¸ªchunks
                try:
                    import time
                    start_time = time.time()
                    response = ollama.embed(model=self.embedding_model, input=chunk)
                    embedding_time = time.time() - start_time
                    
                    if response and 'embeddings' in response:
                        embedding_success += 1
                        embedding_times.append(embedding_time)
                except Exception as e:
                    print(f"âŒ Embedding å¤±è´¥: {e}")
            
            results[chunk_size] = {
                'total_chunks': len(all_chunks),
                'avg_length': np.mean(chunk_lengths),
                'min_length': np.min(chunk_lengths),
                'max_length': np.max(chunk_lengths),
                'avg_words': np.mean(word_counts),
                'embedding_success_rate': embedding_success / min(5, len(all_chunks)),
                'avg_embedding_time': np.mean(embedding_times) if embedding_times else 0,
                'sample_chunks': all_chunks[:3]  # ä¿å­˜æ ·æœ¬
            }
            
            print(f"   ğŸ“Š æ€» chunks: {results[chunk_size]['total_chunks']}")
            print(f"   ğŸ“ å¹³å‡é•¿åº¦: {results[chunk_size]['avg_length']:.1f} å­—ç¬¦")
            print(f"   ğŸ“ å¹³å‡å•è¯æ•°: {results[chunk_size]['avg_words']:.1f}")
            print(f"   âœ… Embedding æˆåŠŸç‡: {results[chunk_size]['embedding_success_rate']:.2%}")
        
        return results
    
    def test_retrieval_quality(self, documents: List[str], chunk_sizes: List[int], 
                             test_queries: List[str]) -> Dict:
        """æµ‹è¯•ä¸åŒ chunk sizes çš„æ£€ç´¢è´¨é‡"""
        
        print("\nğŸ¯ æµ‹è¯•æ£€ç´¢è´¨é‡...")
        retrieval_results = {}
        
        for chunk_size in chunk_sizes:
            print(f"\nğŸ“ æµ‹è¯• chunk size {chunk_size} çš„æ£€ç´¢æ•ˆæœ...")
            
            # åˆ›å»º chunks
            all_chunks = []
            for doc in documents:
                chunks = self.create_chunks(doc, chunk_size)
                all_chunks.extend(chunks)
            
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢æµ‹è¯•æ£€ç´¢
            query_scores = []
            for query in test_queries:
                try:
                    # è·å–æŸ¥è¯¢çš„ embedding
                    query_response = ollama.embed(model=self.embedding_model, input=query)
                    if not query_response or 'embeddings' not in query_response:
                        continue
                    
                    query_embedding = np.array(query_response['embeddings'][0])
                    
                    # è®¡ç®—ä¸æ‰€æœ‰ chunks çš„ç›¸ä¼¼åº¦
                    similarities = []
                    for chunk in all_chunks[:10]:  # é™åˆ¶ä¸ºå‰10ä¸ªchunksä»¥èŠ‚çœæ—¶é—´
                        try:
                            chunk_response = ollama.embed(model=self.embedding_model, input=chunk)
                            if chunk_response and 'embeddings' in chunk_response:
                                chunk_embedding = np.array(chunk_response['embeddings'][0])
                                
                                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                                similarity = np.dot(query_embedding, chunk_embedding) / (
                                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)
                                )
                                similarities.append(similarity)
                        except:
                            continue
                    
                    if similarities:
                        max_similarity = max(similarities)
                        avg_similarity = np.mean(similarities)
                        query_scores.append({
                            'query': query,
                            'max_similarity': max_similarity,
                            'avg_similarity': avg_similarity
                        })
                        
                        print(f"   ğŸ” '{query}': æœ€é«˜ç›¸ä¼¼åº¦ {max_similarity:.3f}")
                
                except Exception as e:
                    print(f"   âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            
            retrieval_results[chunk_size] = {
                'query_scores': query_scores,
                'avg_max_similarity': np.mean([qs['max_similarity'] for qs in query_scores]) if query_scores else 0,
                'avg_avg_similarity': np.mean([qs['avg_similarity'] for qs in query_scores]) if query_scores else 0
            }
        
        return retrieval_results
    
    def recommend_chunk_size(self, analysis_results: Dict, retrieval_results: Dict) -> Dict:
        """åŸºäºåˆ†æç»“æœæ¨èæœ€ä½³ chunk size"""
        
        print("\nğŸ’¡ åˆ†æç»“æœå’Œæ¨è...")
        
        recommendations = {}
        
        for chunk_size, stats in analysis_results.items():
            score = 0
            reasons = []
            
            # è¯„åˆ†æ ‡å‡†
            
            # 1. Embedding æˆåŠŸç‡ (æƒé‡: 30%)
            embedding_score = stats['embedding_success_rate'] * 30
            score += embedding_score
            if stats['embedding_success_rate'] == 1.0:
                reasons.append("âœ… Embedding 100% æˆåŠŸ")
            elif stats['embedding_success_rate'] > 0.8:
                reasons.append("âš ï¸ Embedding æˆåŠŸç‡è‰¯å¥½")
            else:
                reasons.append("âŒ Embedding æˆåŠŸç‡è¾ƒä½")
            
            # 2. Chunk æ•°é‡åˆç†æ€§ (æƒé‡: 20%)
            if 5 <= stats['total_chunks'] <= 20:
                chunk_count_score = 20
                reasons.append("âœ… Chunk æ•°é‡é€‚ä¸­")
            elif stats['total_chunks'] < 5:
                chunk_count_score = 10
                reasons.append("âš ï¸ Chunk æ•°é‡åå°‘")
            else:
                chunk_count_score = 15
                reasons.append("âš ï¸ Chunk æ•°é‡åå¤š")
            score += chunk_count_score
            
            # 3. å¹³å‡é•¿åº¦åˆç†æ€§ (æƒé‡: 25%)
            avg_len = stats['avg_length']
            if 150 <= avg_len <= 300:
                length_score = 25
                reasons.append("âœ… é•¿åº¦é€‚ä¸­")
            elif 100 <= avg_len < 150:
                length_score = 20
                reasons.append("âš ï¸ é•¿åº¦åçŸ­")
            elif 300 < avg_len <= 500:
                length_score = 20
                reasons.append("âš ï¸ é•¿åº¦åé•¿")
            else:
                length_score = 10
                reasons.append("âŒ é•¿åº¦ä¸åˆé€‚")
            score += length_score
            
            # 4. æ£€ç´¢è´¨é‡ (æƒé‡: 25%)
            if chunk_size in retrieval_results:
                retrieval_score = retrieval_results[chunk_size]['avg_max_similarity'] * 25
                score += retrieval_score
                if retrieval_results[chunk_size]['avg_max_similarity'] > 0.7:
                    reasons.append("âœ… æ£€ç´¢è´¨é‡ä¼˜ç§€")
                elif retrieval_results[chunk_size]['avg_max_similarity'] > 0.5:
                    reasons.append("âš ï¸ æ£€ç´¢è´¨é‡è‰¯å¥½")
                else:
                    reasons.append("âŒ æ£€ç´¢è´¨é‡ä¸€èˆ¬")
            
            recommendations[chunk_size] = {
                'score': score,
                'reasons': reasons,
                'stats': stats
            }
        
        # æ‰¾å‡ºæœ€ä½³ chunk size
        best_chunk_size = max(recommendations.keys(), key=lambda k: recommendations[k]['score'])
        
        print(f"\nğŸ† æ¨èçš„ chunk size: {best_chunk_size}")
        print(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {recommendations[best_chunk_size]['score']:.1f}/100")
        print("ğŸ“‹ æ¨èç†ç”±:")
        for reason in recommendations[best_chunk_size]['reasons']:
            print(f"   {reason}")
        
        return {
            'recommended_size': best_chunk_size,
            'all_scores': recommendations,
            'best_score': recommendations[best_chunk_size]['score']
        }

def test_cat_facts_chunking():
    """æµ‹è¯• cat facts çš„æœ€ä½³ chunking ç­–ç•¥"""
    
    print("ğŸ± Cat Facts Chunk Size ä¼˜åŒ–")
    print("="*50)
    
    # Cat facts æ•°æ®
    cat_facts = [
        "Cats can travel at a top speed of approximately 31 mph over short distances.",
        "A cat's hearing is better than a dog's. Cats can hear high-frequency sounds up to 64,000 Hz.",
        "Cats spend 70% of their lives sleeping, which is 13-16 hours a day.",
        "A group of cats is called a clowder, and a group of kittens is called a kindle.",
        "Cats have five toes on their front paws but only four on their back paws.",
        "A cat's purr vibrates at a frequency of 25-50 Hz, which can promote healing.",
        "The oldest known pet cat existed 9,500 years ago in Cyprus.",
        "Cats can make over 100 different vocal sounds, while dogs can only make 10.",
        "A cat's whiskers are roughly as wide as its body and help them judge spaces.",
        "Cats have a third eyelid called a nictitating membrane that protects their eyes."
    ]
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "How fast can cats run?",
        "How much do cats sleep?",
        "What sounds can cats make?",
        "How many toes do cats have?"
    ]
    
    # è¦æµ‹è¯•çš„ chunk sizes
    chunk_sizes_to_test = [100, 150, 200, 250, 300, 400]
    
    optimizer = ChunkSizeOptimizer()
    
    # åˆ†æ chunk sizes
    analysis = optimizer.analyze_chunk_sizes(cat_facts, chunk_sizes_to_test)
    
    # æµ‹è¯•æ£€ç´¢è´¨é‡
    retrieval = optimizer.test_retrieval_quality(cat_facts, chunk_sizes_to_test, test_queries)
    
    # è·å–æ¨è
    recommendation = optimizer.recommend_chunk_size(analysis, retrieval)
    
    return recommendation

if __name__ == "__main__":
    recommendation = test_cat_facts_chunking()
    
    print(f"\nâœ… å»ºè®®å°† config.py ä¸­çš„ CHUNK_SIZE æ›´æ–°ä¸º: {recommendation['recommended_size']}")